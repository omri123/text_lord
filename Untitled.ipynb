{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataframe_dataset import DataFrameDataset, yelp_generator\n",
    "from torchtext import data\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchviz import make_dot\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "701it [00:00, 1069.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12800it [00:02, 5860.76it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_data_iterator():\n",
    "    g = yelp_generator('/home/omribloch/data/yelp/csv/train_encoded_2k.csv', max_examples=6400*2)\n",
    "\n",
    "    id_f = data.Field(sequential=False, use_vocab=False)\n",
    "    stars_f = data.Field(sequential=False, use_vocab=False)\n",
    "    review_f = data.Field(sequential=True, use_vocab=True)\n",
    "\n",
    "    dataset = DataFrameDataset(g, id_f, stars_f, review_f)\n",
    "\n",
    "    review_f.build_vocab(dataset)\n",
    "\n",
    "    train_iter = data.BucketIterator(\n",
    "        dataset=dataset, batch_size=64,\n",
    "        sort_key = lambda x: len(x.review), sort=True, repeat=True)\n",
    "    \n",
    "    return train_iter, dataset, review_f.vocab\n",
    "\n",
    "train_iter, dataset, vocab = get_data_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2272"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "2it [00:00, 11.79it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0\n",
      "tensor(4.0882, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "3it [00:00, 10.51it/s]\u001b[A\n",
      "4it [00:00,  9.65it/s]\u001b[A\n",
      "5it [00:00,  8.76it/s]\u001b[A\n",
      "6it [00:00,  7.76it/s]\u001b[A\n",
      "7it [00:00,  7.00it/s]\u001b[A\n",
      "8it [00:01,  6.48it/s]\u001b[A\n",
      "9it [00:01,  5.72it/s]\u001b[A\n",
      "10it [00:01,  5.58it/s]\u001b[A\n",
      "11it [00:01,  5.16it/s]\u001b[A\n",
      "12it [00:01,  4.73it/s]\u001b[A\n",
      "13it [00:02,  4.51it/s]\u001b[A\n",
      "14it [00:02,  4.30it/s]\u001b[A\n",
      "15it [00:02,  4.05it/s]\u001b[A\n",
      "16it [00:03,  3.99it/s]\u001b[A\n",
      "17it [00:03,  3.81it/s]\u001b[A\n",
      "18it [00:03,  3.84it/s]\u001b[A\n",
      "19it [00:03,  3.77it/s]\u001b[A\n",
      "20it [00:04,  3.65it/s]\u001b[A\n",
      "21it [00:04,  3.53it/s]\u001b[A\n",
      "22it [00:04,  3.47it/s]\u001b[A\n",
      "23it [00:05,  3.40it/s]\u001b[A\n",
      "24it [00:05,  3.31it/s]\u001b[A\n",
      "25it [00:05,  3.23it/s]\u001b[A\n",
      "26it [00:06,  3.15it/s]\u001b[A\n",
      "27it [00:06,  3.08it/s]\u001b[A\n",
      "28it [00:06,  2.95it/s]\u001b[A\n",
      "29it [00:07,  2.87it/s]\u001b[A\n",
      "30it [00:07,  2.78it/s]\u001b[A\n",
      "31it [00:07,  2.75it/s]\u001b[A\n",
      "32it [00:08,  2.72it/s]\u001b[A\n",
      "33it [00:08,  2.70it/s]\u001b[A\n",
      "34it [00:09,  2.60it/s]\u001b[A\n",
      "35it [00:09,  2.60it/s]\u001b[A\n",
      "36it [00:09,  2.51it/s]\u001b[A\n",
      "37it [00:10,  2.45it/s]\u001b[A\n",
      "38it [00:10,  2.40it/s]\u001b[A\n",
      "39it [00:11,  2.38it/s]\u001b[A\n",
      "40it [00:11,  2.33it/s]\u001b[A\n",
      "41it [00:12,  2.27it/s]\u001b[A\n",
      "42it [00:12,  2.23it/s]\u001b[A\n",
      "43it [00:13,  2.19it/s]\u001b[A\n",
      "44it [00:13,  2.16it/s]\u001b[A\n",
      "45it [00:13,  2.18it/s]\u001b[A\n",
      "46it [00:14,  2.18it/s]\u001b[A\n",
      "47it [00:14,  2.18it/s]\u001b[A\n",
      "48it [00:15,  2.15it/s]\u001b[A\n",
      "49it [00:15,  2.12it/s]\u001b[A\n",
      "50it [00:16,  2.14it/s]\u001b[A\n",
      "51it [00:16,  2.10it/s]\u001b[A\n",
      "52it [00:17,  2.05it/s]\u001b[A\n",
      "53it [00:17,  2.03it/s]\u001b[A\n",
      "54it [00:18,  2.00it/s]\u001b[A\n",
      "55it [00:18,  1.98it/s]\u001b[A\n",
      "56it [00:19,  1.95it/s]\u001b[A\n",
      "57it [00:19,  1.97it/s]\u001b[A\n",
      "58it [00:20,  1.95it/s]\u001b[A\n",
      "59it [00:20,  1.92it/s]\u001b[A\n",
      "60it [00:21,  1.89it/s]\u001b[A\n",
      "61it [00:22,  1.88it/s]\u001b[A\n",
      "62it [00:22,  1.83it/s]\u001b[A\n",
      "63it [00:23,  1.80it/s]\u001b[A\n",
      "64it [00:23,  1.79it/s]\u001b[A\n",
      "65it [00:24,  1.78it/s]\u001b[A\n",
      "66it [00:24,  1.74it/s]\u001b[A\n",
      "67it [00:25,  1.73it/s]\u001b[A\n",
      "68it [00:26,  1.70it/s]\u001b[A\n",
      "69it [00:26,  1.69it/s]\u001b[A\n",
      "70it [00:27,  1.67it/s]\u001b[A\n",
      "71it [00:27,  1.68it/s]\u001b[A\n",
      "72it [00:28,  1.66it/s]\u001b[A\n",
      "73it [00:29,  1.65it/s]\u001b[A\n",
      "74it [00:29,  1.64it/s]\u001b[A\n",
      "75it [00:30,  1.59it/s]\u001b[A\n",
      "76it [00:31,  1.59it/s]\u001b[A\n",
      "77it [00:31,  1.59it/s]\u001b[A\n",
      "78it [00:32,  1.59it/s]\u001b[A\n",
      "79it [00:32,  1.58it/s]\u001b[A\n",
      "80it [00:33,  1.57it/s]\u001b[A\n",
      "81it [00:34,  1.55it/s]\u001b[A\n",
      "82it [00:34,  1.51it/s]\u001b[A\n",
      "83it [00:35,  1.49it/s]\u001b[A\n",
      "84it [00:36,  1.48it/s]\u001b[A\n",
      "85it [00:37,  1.48it/s]\u001b[A\n",
      "86it [00:37,  1.47it/s]\u001b[A\n",
      "87it [00:38,  1.43it/s]\u001b[A\n",
      "88it [00:39,  1.42it/s]\u001b[A\n",
      "89it [00:39,  1.43it/s]\u001b[A\n",
      "90it [00:40,  1.42it/s]\u001b[A\n",
      "91it [00:41,  1.43it/s]\u001b[A\n",
      "92it [00:41,  1.41it/s]\u001b[A\n",
      "93it [00:42,  1.41it/s]\u001b[A\n",
      "94it [00:43,  1.40it/s]\u001b[A\n",
      "95it [00:44,  1.38it/s]\u001b[A\n",
      "96it [00:44,  1.37it/s]\u001b[A\n",
      "97it [00:45,  1.36it/s]\u001b[A\n",
      "98it [00:46,  1.35it/s]\u001b[A\n",
      "99it [00:47,  1.33it/s]\u001b[A\n",
      "100it [00:47,  1.32it/s]\u001b[A\n",
      "101it [00:48,  1.31it/s]\u001b[A\n",
      "102it [00:49,  1.30it/s]\u001b[A\n",
      "103it [00:50,  1.28it/s]\u001b[A\n",
      "104it [00:51,  1.28it/s]\u001b[A\n",
      "105it [00:51,  1.27it/s]\u001b[A\n",
      "106it [00:52,  1.26it/s]\u001b[A\n",
      "107it [00:53,  1.25it/s]\u001b[A\n",
      "108it [00:54,  1.22it/s]\u001b[A\n",
      "109it [00:55,  1.21it/s]\u001b[A\n",
      "110it [00:56,  1.21it/s]\u001b[A\n",
      "111it [00:56,  1.20it/s]\u001b[A\n",
      "112it [00:57,  1.20it/s]\u001b[A\n",
      "113it [00:58,  1.19it/s]\u001b[A\n",
      "114it [00:59,  1.16it/s]\u001b[A\n",
      "115it [01:00,  1.16it/s]\u001b[A\n",
      "116it [01:01,  1.17it/s]\u001b[A\n",
      "117it [01:02,  1.15it/s]\u001b[A\n",
      "118it [01:03,  1.14it/s]\u001b[A\n",
      "119it [01:03,  1.14it/s]\u001b[A\n",
      "120it [01:04,  1.13it/s]\u001b[A\n",
      "121it [01:05,  1.13it/s]\u001b[A\n",
      "122it [01:06,  1.12it/s]\u001b[A\n",
      "123it [01:07,  1.11it/s]\u001b[A\n",
      "124it [01:08,  1.09it/s]\u001b[A\n",
      "125it [01:09,  1.09it/s]\u001b[A\n",
      "126it [01:10,  1.08it/s]\u001b[A\n",
      "127it [01:11,  1.06it/s]\u001b[A\n",
      "128it [01:12,  1.05it/s]\u001b[A\n",
      "129it [01:13,  1.05it/s]\u001b[A\n",
      "130it [01:14,  1.04it/s]\u001b[A\n",
      "131it [01:15,  1.03it/s]\u001b[A\n",
      "132it [01:16,  1.02it/s]\u001b[A\n",
      "133it [01:17,  1.01it/s]\u001b[A\n",
      "134it [01:18,  1.00it/s]\u001b[A\n",
      "135it [01:19,  1.01s/it]\u001b[A\n",
      "136it [01:20,  1.02s/it]\u001b[A\n",
      "137it [01:21,  1.03s/it]\u001b[A\n",
      "138it [01:22,  1.04s/it]\u001b[A\n",
      "139it [01:23,  1.05s/it]\u001b[A\n",
      "140it [01:24,  1.07s/it]\u001b[A\n",
      "141it [01:25,  1.07s/it]\u001b[A\n",
      "142it [01:26,  1.08s/it]\u001b[A\n",
      "143it [01:27,  1.09s/it]\u001b[A\n",
      "144it [01:29,  1.10s/it]\u001b[A\n",
      "145it [01:30,  1.11s/it]\u001b[A\n",
      "146it [01:31,  1.12s/it]\u001b[A\n",
      "147it [01:32,  1.13s/it]\u001b[A\n",
      "148it [01:33,  1.14s/it]\u001b[A\n",
      "149it [01:34,  1.18s/it]\u001b[A\n",
      "150it [01:36,  1.18s/it]\u001b[A"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-64c008baac43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreviews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mprediction_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Zion/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-52-39f362dcee07>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sentences, stars, sample_ids)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mlstm_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_embeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mprobabilities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Zion/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Zion/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    562\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_packed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Zion/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward_tensor\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0munsorted_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munsorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Zion/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward_impl\u001b[0;34m(self, input, hx, batch_sizes, max_batch_size, sorted_indices)\u001b[0m\n\u001b[1;32m    524\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m             result = _VF.lstm(input, hx, self._get_flat_weights(), self.bias, self.num_layers,\n\u001b[0;32m--> 526\u001b[0;31m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    527\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._get_flat_weights(), self.bias,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "PAD = len(vocab)\n",
    "\n",
    "\n",
    "def shift_left(tensor, padding_value):\n",
    "    \"\"\"\n",
    "    tensor is 2-d, (sequence, batch)\n",
    "    we are shifting the sequence and we will get (sequence+1, batch)\n",
    "    \"\"\"\n",
    "    assert len(tensor.size()) == 2\n",
    "    new_tensor = torch.full(tensor.size(), padding_value, dtype=torch.int64)\n",
    "    new_tensor[0:-1, :] = tensor[1:, :]\n",
    "    return new_tensor\n",
    "\n",
    "\n",
    "class LSTM_LORD(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, vocab_size, number_of_samples):\n",
    "        super(LSTM_LORD, self).__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, dim)\n",
    "        self.stars_embedding = nn.Embedding(6, dim)\n",
    "        self.sample_embedding = nn.Embedding(number_of_samples, dim)\n",
    "        \n",
    "        # the LSTM itself\n",
    "        self.lstm = nn.LSTM(dim, dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to word space\n",
    "        self.fc = nn.Linear(dim, vocab_size)\n",
    "\n",
    "    def forward(self, sentences, stars, sample_ids):\n",
    "        # sentences are shifted        \n",
    "        \n",
    "        w_embeds = self.word_embeddings(sentences)\n",
    "        s_embeds = self.stars_embedding(stars).unsqueeze_(0)\n",
    "        id_embeds = self.sample_embedding(sample_ids).unsqueeze_(0)\n",
    "        tmp = s_embeds + id_embeds\n",
    "        \n",
    "#         embeds = w_embeds + s_embeds + id_embeds\n",
    "        \n",
    "        \n",
    "        lstm_out, _ = self.lstm(w_embeds, (tmp, tmp))\n",
    "        logits = self.fc(lstm_out)\n",
    "        probabilities = F.log_softmax(logits, dim=1)\n",
    "        return probabilities\n",
    "    \n",
    "#model = LSTM_LORD(dim=64, vocab_size=len(vocab)+1, number_of_samples=6400*2)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "    \n",
    "\n",
    "# training loop\n",
    "from tqdm import tqdm\n",
    "for i, batch in tqdm(enumerate(train_iter)):\n",
    "    ids = batch.id\n",
    "    stars = batch.stars\n",
    "    reviews = batch.review\n",
    "    \n",
    "    model.zero_grad()\n",
    "    \n",
    "    predictions = model(reviews, stars, ids)\n",
    "    \n",
    "    prediction_flat = predictions.view(-1, len(vocab)+1)\n",
    "    targets_flat = shift_left(reviews, PAD).view(-1)\n",
    "    \n",
    "    loss = loss_function(prediction_flat, targets_flat)\n",
    "    if i % 200 == 0:\n",
    "        print('epoch = {}'.format(i // 200))\n",
    "        print(loss)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
