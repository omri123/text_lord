{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairseq.models import FairseqEncoder, FairseqEncoderDecoderModel\n",
    "\n",
    "from fairseq.models.fconv import (\n",
    "    Embedding,\n",
    "    PositionalEmbedding,\n",
    "    FConvDecoder\n",
    ")\n",
    "from torch.distributions import Normal\n",
    "\n",
    "\n",
    "from restorant_dataset import RestDataset, lines_generator\n",
    "from torchtext import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoEncoder(FairseqEncoder):\n",
    "    \"\"\"\n",
    "    The input contain:\n",
    "        sequence of latent embedding indecies\n",
    "        class index (positive / negative)\n",
    "        embed the input and noise the sample embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, sample_size, padding_index, ntokens=5, embed_dim=512, noise_std=0.1, dropout=0.1):\n",
    "        \"\"\"\n",
    "        number of latent-space tokens is constant.\n",
    "        \"\"\"\n",
    "        super().__init__(None)\n",
    "        self.dropout = dropout\n",
    "        self.dim = embed_dim\n",
    "        self.ntokens = ntokens\n",
    "        \n",
    "        self.content_embeddings = Embedding(sample_size, embed_dim * ntokens, padding_idx) # tokens-encoder, sample-specific\n",
    "        \n",
    "        self.negative_embedding = PositionalEmbedding(num_embeddings=ntokens, \n",
    "                                                     embedding_dim=embed_dim, \n",
    "                                                     padding_idx=padding_index)\n",
    "        \n",
    "        self.positive_embedding = PositionalEmbedding(num_embeddings=ntokens, \n",
    "                                                     embedding_dim=embed_dim, \n",
    "                                                     padding_idx=padding_index)\n",
    "        \n",
    "        self.noise = Normal(loc=0.0, scale=noise_std)\n",
    "        \n",
    "    def forward(self, src_tokens, src_lengths):\n",
    "        \"\"\"\n",
    "        src_tokens are two: one for the sentiment (0 or 1),\n",
    "                            and one for the sample [0.. sample_size]\n",
    "                            shape is always (batch, 2)\n",
    "        src_lengths is (batch)-size array full of 2.\n",
    "        \"\"\"\n",
    "        \n",
    "        # content embedding and noise\n",
    "        content = self.content_embeddings(src_tokens[:, 1])\n",
    "        content = torch.view(self.ntokens, self.dim)\n",
    "        content = content + self.noise.sample(sample_shape=content.size())\n",
    "        \n",
    "        # sentiment positional embedding\n",
    "        positions = torch.arange(0, self.ntokens).unsqueeze_(0) # 1 x ntokens\n",
    "        sentiment = src_tokens[:, 0].unsqueeze_(1).unsqueeze_(2) # batch x 1 x 1\n",
    "        sentiment = self.positive_embedding(positions) * sentiment + \\\n",
    "                     self.negative_embedding(positions) * (torch.tensor(1) - sentiment) # batch x ntokens x dim\n",
    "        \n",
    "        x = content + sentiment\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        return {\n",
    "            'encoder_out': (x,x),\n",
    "            'encoder_padding_mask': None\n",
    "        }\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoEncoderFConvDecoderModel(FairseqEncoderDecoderModel):\n",
    "    \"\"\"\n",
    "    encoder-decoder that use the no-encoder as encoder and the fconv decoder as decoder.\n",
    "    inspiration from fconv.py\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__(encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "49190it [00:00, 10.97it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_dataset(max_examples):\n",
    "    g = lines_generator()\n",
    "\n",
    "    id_f = data.Field(sequential=False, use_vocab=False)\n",
    "    stars_f = data.Field(sequential=False, use_vocab=False)\n",
    "    review_f = data.Field(sequential=True, use_vocab=True)\n",
    "\n",
    "    dataset = RestDataset(g, id_f, stars_f, review_f, max_examples)\n",
    "\n",
    "    review_f.build_vocab(dataset)\n",
    "    \n",
    "    return dataset, review_f.vocab\n",
    "\n",
    "nsamples = 50000\n",
    "dataset, vocab = get_dataset(50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "781\n",
      "7590\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "iterations_per_epoch = nsamples // batch_size\n",
    "print(iterations_per_epoch)\n",
    "print(len(vocab))\n",
    "train_iter = data.BucketIterator(\n",
    "        dataset=dataset, batch_size=batch_size,\n",
    "        sort_key = lambda x: len(x.review), sort=True, repeat=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkpoint(model, path='/cs/labs/dshahaf/omribloch/train/text_lord/checkpoint.txt'):\n",
    "#     Save:\n",
    "    if os.path.exists(path):\n",
    "        os.remove(path)\n",
    "    torch.save(model.state_dict(), path)\n",
    "    print('saved checkpoint!')\n",
    "\n",
    "def load_checkpoint(path='/cs/labs/dshahaf/omribloch/train/text_lord/checkpoint.txt'):\n",
    "#     Load:\n",
    "    model = TheModelClass(*args, **kwargs)\n",
    "    model.load_state_dict(torch.load(PATH))\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_dictionary = fairseq.data.dictionary()\n",
    "for token in vocab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = NoEncoder(nsamples, PAD)\n",
    "decoder = FConvDecoder()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fairseq",
   "language": "python",
   "name": "fairseq"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
